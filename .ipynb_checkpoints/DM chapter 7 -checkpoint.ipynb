{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "consumer_key = \"<Your Consumer Key Here>\"\n",
    "consumer_secret = \"<Your Consumer Secret Here>\"\n",
    "access_token = \"<Your Access Token Here>\"\n",
    "access_token_secret = \"<Your Access Token Secret Here>\"\n",
    "authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)\n",
    "t = twitter.Twitter(auth=authorization, retry=True)\n",
    "\n",
    "import os\n",
    "data_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"twitter\")\n",
    "output_filename = os.path.join(data_folder, \"python_tweets.json\")\n",
    "import json\n",
    "original_users = []\n",
    "tweets = []\n",
    "user_ids = {}\n",
    "'''搜索包含python的消息，遍历搜索结果：'''\n",
    "search_results = t.search.tweets(q=\"python\", count=100)['statuses']\n",
    "for tweet in search_results:\n",
    "    if 'text' in tweet:\n",
    "        original_users.append(tweet['user']['screen_name'])\n",
    "        user_ids[tweet['user']['screen_name']] = tweet['user']['id']\n",
    "        tweets.append(tweet['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return [{word: True for word in word_tokenize(document)} for document in X]\n",
    "'''加载第六章的分类模型'''    \n",
    "from sklearn.externals import joblib\n",
    "model_filename = os.path.join(os.path.expanduser(\"~\"), \"Models\", \"twitter\", \"python_context.pkl\")\n",
    "context_classifier = joblib.load(model_filename)\n",
    "y_pred = context_classifier.predict(tweets)\n",
    "relevant_tweets = [tweets[i] for i in range(len(tweets)) if y_pred[i] == 1]\n",
    "relevant_users = [original_users[i] for i in range(len(tweets)) if y_pred[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  调用相应API：friends/ids获取好友信息\n",
    "import time\n",
    "def get_friends(t, user_id):\n",
    "    friends = []\n",
    "    cursor = -1 \n",
    "    #当向Twitter请求数据时，不仅返回所需数据，还返回数据类型为整数的游标， Twitter用游标跟踪每一次请求。\n",
    "    #如果没有更多内容，游标为0；否则，可以使用游标获得下一页数据。开始把游标设置为1，表明是数据的开始：\n",
    "    while cursor != 0:\n",
    "        try:\n",
    "            results = t.friends.ids(user_id=user_id, cursor=cursor, count=5000)\n",
    "            friends.extend([friend for friend in results['ids']])\n",
    "            cursor = results['next_cursor']\n",
    "            if len(friends) >= 10000:\n",
    "                break\n",
    "        except TypeError as e:\n",
    "            if results is None:\n",
    "                print(\"You probably reached your API limit, waiting for 5 minutes\")\n",
    "                sys.stdout.flush()\n",
    "                time.sleep(5*60) # 5 minute wait\n",
    "            else:\n",
    "                raise e\n",
    "        except twitter.TwitterHTTPError as e:\n",
    "            break\n",
    "        finally:\n",
    "            time.sleep(60)\n",
    "    return friends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  构建网络\n",
    "friends = {}\n",
    "'''遍历用户名'''\n",
    "for screen_name in relevant_users:\n",
    "    '''拿到用户名的ID号'''\n",
    "    user_id = user_ids[screen_name]\n",
    "    '''建立朋友字典'''\n",
    "    friends[user_id] = get_friends(t, user_id)\n",
    "'''删除没有朋友的ID号'''\n",
    "friends = {user_id:friends[user_id] for user_id in friends if len(friends[user_id]) > 0}\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "def count_friends(friends):\n",
    "    friend_count = defaultdict(int)\n",
    "    for friend_list in friends.values():\n",
    "        for friend in friend_list:\n",
    "            '''被多少人关注'''\n",
    "            friend_count[friend] += 1\n",
    "    return friend_count\n",
    "\n",
    "friend_count= count_friends(friends)\n",
    "from operator import itemgetter\n",
    "'''对关注数排序'''\n",
    "best_friends = sorted(friend_count.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "'''建立一循环，凑够150个用户的好友数据后，该循环就会结束。遍历best_friends字典（按照在现有用户中的好友数多少排序），\n",
    "找到还没有获取好友列表的用户，然后获取他的好友列表，更新friends列表。最后，再次查找谁是最受欢迎的'''\n",
    "while len(friends) < 150:\n",
    "    for user_id, count in best_friends:\n",
    "        if user_id not in friends:\n",
    "            break\n",
    "    friends[user_id] = get_friends(t, user_id)\n",
    "    for friend in friends[user_id]:\n",
    "        friend_count[friend] += 1\n",
    "    best_friends = sorted(friend_count.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "'''采集数据大约要运行2h，最好把中间结果保存下来，因为有时不得不关闭计算机。使用json库，就可以轻松把好友字典保存到文件里：'''\n",
    "import json\n",
    "friends_filename = os.path.join(data_folder, \"python_friends.json\")\n",
    "with open(friends_filename, 'w') as outf:\n",
    "    json.dump(friends, outf)\n",
    "'''使用json.load函数，从文件中加载数据：'''    \n",
    "with open(friends_filename) as inf:\n",
    "    friends = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  创建图\n",
    "import networkx as nx\n",
    "G = nx.DiGraph()\n",
    "'''只将150名核心用户彼此间的好友关系绘制成图像，其他好友关系由于数据量很大难以可视化。把核心用户作为顶点，添加到图中。'''\n",
    "main_users = friends.keys()\n",
    "G.add_nodes_from(main_users)\n",
    "'''接着要创建边。如果第二个用户是第一个用户的好友，那么就在这两个顶点之间建立一条边。遍历所有的核心用户：'''\n",
    "for user_id in friends:\n",
    "    for friend in friends[user_id]:\n",
    "        if friend in main_users:\n",
    "            G.add_edge(user_id, friend)\n",
    "            \n",
    "%matplotlib inline\n",
    "nx.draw(G)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(3,figsize=(20,20))\n",
    "nx.draw(G, alpha=0.1, edge_color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-864bcca17f6b>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-864bcca17f6b>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    return len(friends1 & friends2) / len(friends1 | friends2)\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "##  创建用户相似度图\n",
    "friends = {user: set(friends[user]) for user in friends}\n",
    "def compute_similarity(friends1, friends2):\n",
    "    return len(friends1 & friends2) / len(friends1 | friends2) #杰卡德相似系数（Jaccard Similarity）\n",
    "\n",
    "def create_graph(friends, threshold=0):\n",
    "    G = nx.Graph()\n",
    "    for user1 in friends.keys():\n",
    "        for user2 in friends.keys():\n",
    "            if user1 == user2:\n",
    "                continue\n",
    "                weight = compute_similarity(friends[user1], friends[user2])\n",
    "            '''画图'''\n",
    "            if weight >= threshold:\n",
    "                G.add_node(user1)\n",
    "                G.add_node(user2)\n",
    "                G.add_edge(user1, user2, weight=weight)\n",
    "    return G\n",
    "G = create_graph(friends)\n",
    "plt.figure(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''使用spring_layout布局方法：'''\n",
    "pos = nx.spring_layout(G)\n",
    "'''使用pos布局方法，确定顶点位置：'''\n",
    "nx.draw_networkx_nodes(G, pos)\n",
    "'''接下来，绘制边。遍历图中的每条边，获得其权重：'''\n",
    "edgewidth = [ d['weight'] for (u,v,d) in G.edges(data=True)]\n",
    "'''绘制各条边：'''\n",
    "nx.draw_networkx_edges(G, pos, width=edgewidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  寻找子图\n",
    "##  连同分支\n",
    "G = create_graph(friends, 0.1)\n",
    "sub_graphs = nx.connected_component_subgraphs(G)\n",
    "for i, sub_graph in enumerate(sub_graphs):\n",
    "    n_nodes = len(sub_graph.nodes())\n",
    "    print(\"Subgraph {0} has {1} nodes\".format(i, n_nodes))\n",
    "G = create_graph(friends, 0.25)\n",
    "sub_graphs = nx.connected_component_subgraphs(G)\n",
    "for i, sub_graph in enumerate(sub_graphs):\n",
    "    n_nodes = len(sub_graph.nodes())\n",
    "    print(\"Subgraph {0} has {1} nodes\".format(i, n_nodes))\n",
    "    \n",
    "'''可以用不同的颜色把所有连通分支都画出来。因为各连通分支之间没有连接，因此没必要把它们画到一张图中。\n",
    "\n",
    "sub_graphs是生成器而不是连通分支列表。用nx.number_connected_components找出连通分支的总数；NetworkX无法使用len函数。\n",
    "'''\n",
    "sub_graphs = nx.connected_component_subgraphs(G)\n",
    "n_subgraphs = nx.number_connected_components(G)\n",
    "fig = plt.figure(figsize=(20, (n_subgraphs * 3)))\n",
    "for i, sub_graph in enumerate(sub_graphs):\n",
    "    ax = fig.add_subplot(int(n_subgraphs / 3), 3, i)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    '''绘制顶点和边（用ax参数绘制相应的子图）。绘图之前需要设置好布局：'''\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, sub_graph.nodes(), ax=ax, node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, sub_graph.edges(), ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  寻找子图\n",
    "##  优化参数选取准则\n",
    "def compute_silhouette(threshold, friends):\n",
    "    G = create_graph(friends, threshold=threshold)\n",
    "    if len(G.nodes()) < 2: #检查顶点数是否小于2\n",
    "        return -99\n",
    "    sub_graphs = nx.connected_component_subgraphs(G)\n",
    "    if not (2 <= nx.number_connected_components() < len(G.nodes()) - 1): \n",
    "        #轮廓系数的定义还要求至少有两个连通分支（才能计算不同簇之间的距离），并且至少其中一个连通分支有两个顶点（计算簇内距离）。\n",
    "        return -99\n",
    "    \n",
    "    '''需要获取标识着顶点被分到哪个连通分支的标签。遍历所有的连通分支，用字典保存顶点及其所属的连通分支'''\n",
    "    label_dict = {}\n",
    "    for i, sub_graph in enumerate(sub_graphs):\n",
    "        for node in sub_graph.nodes():\n",
    "            label_dict[node] = i\n",
    "    '''遍历图中所有顶点，依次获取到每个顶点的标签。需要分两步来做，先按一定顺序取到图，再遍历。因为图中顶点没有明确的顺序，\n",
    "    但是只要没有改动图，顶点会维持现有顺序。这就表明，只要没有改动图，在图上调用.nodes()方法，返回的顶点顺序总是一致的。代码如下：'''\n",
    "    labels = np.array([label_dict[node] for node in G.nodes()])\n",
    "    '''轮廓系数函数接收的是距离矩阵，因此，要想办法把图转换为矩阵。首先，使用NetworkX的to_scipy_sparse_matrix函数把图转换为矩阵形式：'''\n",
    "    X = nx.to_scipy_sparse_matrix(G).todense()\n",
    "    X = 1 - X\n",
    "    return silhouette_score(X, labels, metric='precomputed')\n",
    "def inverted_silhouette(threshold, friends):\n",
    "    return -compute_silhouette(threshold, friends)\n",
    "result = minimize(inverted_silhouette, 0.1, args=(friends,))\n",
    "'''\n",
    " inverted_silhouette： 对我们要最小化的函数compute_silhouette进行取反操作，将其变为损失函数。\n",
    " 0.1：我们一开始猜测阈值为0.1时，函数取到最小值。\n",
    " options={'maxiter':10}：只进行10轮迭代（增加迭代次数，效果可能更好，但运行时间也会相应增加）。\n",
    " method='nelder-mead'：使用下山单纯形法（Nelder-Mead）优化方法（SciPy提供的优化方法）。\n",
    " args=(friends,)：向被优化的函数传入friends字典参数。\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
